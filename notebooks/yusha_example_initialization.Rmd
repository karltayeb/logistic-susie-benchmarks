---
title: "Yusha Example"
author: "Karl Tayeb"
date: "2023-01-09"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
    toc_collapsed: true
    toc_depth: 3
    number_sections: true
---

```{r setup}
library(targets)
library(dplyr) 
library(tidyr)
library(logisticsusie)
```

```{r}
print_cs_tbl <- function(fm){
  cs_tbl <- fits2 %>%
    filter(fit_method == fm) %>%
    {.$cs_tbl[[1]]}

  lbf <- fits2 %>%
    filter(fit_method == fm) %>%
    {.$lbf[[1]]} 

  cs_tbl$lbf <- lbf
  cs_tbl %>% select(c(size, top_feaure, top_feature_alpha, lbf, L))
}
```

## Top 500 Gene Sets

```{r}
tar_load(yusha_data500)
```

```{r}
ser_uvb <- with(yusha_data500, logisticsusie::fit_uvb_ser(X, y))

# compute BF1
ser_uvb$lbf_model

# mean(BF_i)
log(mean(exp(ser_uvb$lbf)))

# ELBO - p(y | H0)
with(ser_uvb, {
  p <- length(alpha)
  sum(alpha * elbo) - logisticsusie:::categorical_kl(alpha, rep(1/p, p)) - null_loglik
})
```


We are going to initialize IBSS in a way that is similar to a forward regression.
After fitting the SER, we are going to change $q(\gamma)$ to 

$$
\tilde q(\gamma) \propto q(\gamma)^{1/T}
$$
For $0 < T < 1$ this will make $\tilde q(\gamma)$ peaky around its max. For $T$ close to zero, it essentially picks the variable with the highest posterior probability. 

```{r}
# annealed ELBO - p(y | H0)
compute_annealed_lbf <- function(ser, temperature){
  with(ser, {
    p <- length(alpha)
    alpha2 <- logisticsusie:::anneal(alpha, temperature)
    sum(alpha2 * elbo) - logisticsusie:::categorical_kl(alpha2, rep(1/p, p)) - null_loglik
  })
}

annealed_lbf <- purrr::map_dbl(seq(0.1, 2, by=0.1), ~compute_annealed_lbf(ser_uvb, .x))
plot(seq(0.1, 2, by=0.1), annealed_lbf, xlab='Temerature', ylab='SER logBF')

ser_uvbser_uvb$elbo - ser_uvb$null_loglik
```


```{r}
ibss_uvb <- with(yusha_data500, logisticsusie::ibss_from_ser2(
  X, y, L=5, 
  maxit = 10,
  ser_function = logisticsusie::fit_uvb_ser,
  save_history = T,
  tol = 1e-6
))

make_cs_summary_tbl <- function(fit){
  fit %>%
    {logisticsusie:::cs_tbl2(.$alpha)} %>%
    select(L, top_feaure, top_feature_alpha, size) %>%
    mutate(lbf = fit$lbf_ser)
}

ibss_uvb$history[[2]] %>%
  logisticsusie:::ibss_post_process() %>%
  make_cs_summary_tbl()

ibss_uvb$history[[9]] %>%
  logisticsusie:::ibss_post_process() %>%
  make_cs_summary_tbl()
```


```{r}
# make first few iterations "peaky" to get a good initialization
ibss_uvb_warmup <- with(yusha_data500, logisticsusie::ibss_from_ser2(
  X, y, L=5, 
  maxit = 10,
  ser_function = logisticsusie::fit_uvb_ser,
  save_history = T,
  temperature = seq(0.1, 1.0, by=0.1)
))


ibss_uvb_warmup$history[[9]] %>%
  logisticsusie:::ibss_post_process() %>%
  make_cs_summary_tbl()


ibss_uvb_warmup$history[[9]] %>%
  logisticsusie:::ibss_post_process() %>%
  make_cs_summary_tbl()
```

```{r}
# initialize from uvb_warmup, fit to convergence
ibss_uvb_warm_start <- with(yusha_data500, logisticsusie::ibss_from_ser2(
  X, y, L=5, 
  maxit = 10,
  ser_function = logisticsusie::fit_uvb_ser,
  save_history = T,
  q_init = ibss_uvb_warmup$history[[9]]
))

ibss_uvb_warm_start$history[[5]] %>%
  logisticsusie:::ibss_post_process() %>%
  make_cs_summary_tbl()
```

```{r}
ibss_uvb %>% make_cs_summary_tbl()
```

```{r}
process <- logisticsusie:::ibss_post_process
compute_pip <- logisticsusie:::compute_pip
compute_cs <- logisticsusie:::get_all_cs2

cat_alpha <- function(sers){
  do.call(rbind, (purrr::map(1:L, ~purrr::pluck(sers[[.x]], 'alpha'))))
}

# get top cs per feature
susie_plot2 <- function(alpha, ...){
  p <- ncol(alpha)
  L <- nrow(alpha)
  colors <- brewer.pal(L, 'Set1')
  
  pip <- alpha %>% compute_pip
  sets <- compute_cs(alpha)
  top_set <- purrr::map_int(1:p, ~which.max(alpha[, .x]))
  
  plot(1, type="n",
       xlab="Variable", ylab="PIP",
       xlim=c(1, p), ylim=c(0, 1), ...
      )
  
  # plot points not in CS
  no_cs <- setdiff(1:p, unique(unlist(purrr::map(1:L, ~purrr::pluck(sets[[.x]], 'cs')))))
  points(no_cs, pip[no_cs])
  
  # plot CSs
  for(l in 1:length(sets)){
    x <- intersect(which(top_set == l), sets[[l]]$cs)
    points(x, pip[x], col=colors[[l]], pch=19)
  }

}

pip_animation <- function(history, name='pip_animation.gif'){
  niter <- length(history)
  for(i in 1:niter){
    png(file=paste0(i, '.png'), height = 800, width=1000, res=200)
    history[[i]] %>%
      cat_alpha %>%
      susie_plot2(main = paste0('Iter = ', i - 1))
    dev.off()
  }
  command <- paste0("convert -delay 100 *.png ", name)
  print(command)
  system(command)
  file.remove(list.files(pattern=".png"))
}

pip_animation(ibss_uvb$history, name='ibss_uvb.gif')
pip_animation(ibss_uvb_warmup$history, name='ibss_uvb_warmup.gif')
pip_animation(ibss_uvb_warm_start$history, name='ibss_uvb_warm_start.gif')
```


```{r}
par(mfrow=c(1, 3))

tail(ibss_uvb$history, 1)[[1]] %>%
  {process(.)$alpha} %>%
  susie_plot2(main = 'Prior Initialization')

tail(ibss_uvb_warmup$history, 1)[[1]] %>%
  {process(.)$alpha} %>%
  susie_plot2(main = 'Forward selection initialzation')

tail(ibss_uvb_warm_start$history, 1)[[1]] %>%
  {process(.)$alpha} %>%
  susie_plot2(main = 'Iter = 1')
```




### Annealing (Forard regression initialization)

```{r}
tar_load(yusha_data500)
yusha_data500

temp_schedule <- c(seq(0.1, 1, by = 0.1), rep(1, 10))
temp_schedule <- c(0.01, 0.01, 0.01)

l1 <- with(yusha_data500, logisticsusie::fit_uvb_ser(as.matrix(X), y))
max(logisticsusie:::anneal(l1$alpha, 0.01))

res <- with(yusha_data500, logisticsusie::ibss_from_ser2(
  X, y, L=3, 
  maxit = 2,
  ser_function = logisticsusie::fit_uvb_ser,
  temperature = temp_schedule
))

logisticsusie:::cs_tbl2(res$alpha) %>%
  unnest_longer(c(cs, prob))


make_cs
```

```{r}

```


### SERs

The Naive VB SER puts all it's mass on the 33rd most marginally enriched feature, and has an extremely small BF.
We know there are several strong enrichments in this example, highlighting how weak the VB bound is when we are forced to optimize the bound over all features jointly/on average.

Similar to when we looked at all gene sets, we see that UVB-SER gives a log BF closest to the exact result, while GLM-SER dramatically overeseimtates the log BF.

```{r}
# SERs
rbind(
  print_cs_tbl('vb_ser') %>% mutate(method = 'vb_ser'),
  print_cs_tbl('glm_ser')  %>% mutate(method = 'glm_ser'),
  print_cs_tbl('uvb_ser') %>% mutate(method = 'uvb_ser'),
  print_cs_tbl('bayes_ser')  %>% mutate(method = 'bayes_ser')
)
```

GLM SER dramatically overestimates the the evidence for an effect, while VB-SER dramatically underestimates the evidence for an effect.
This indicates that 


```{r}
# SERs
rbind(
  print_cs_tbl('linear_susie_L10') %>% mutate(method = 'linear_susie_L10'),
  print_cs_tbl('binsusie_L10')  %>% mutate(method = 'binsusie_L10'),
  print_cs_tbl('ibss_uvb_L10') %>% mutate(method = 'ibss_uvb_L10')
)
print_cs_tbl('linear_susie_L10') %>% mutate(method = 'linear_susie_L10')
print_cs_tbl('binsusie_L10')  %>% mutate(method = 'binsusie_L10')
print_cs_tbl('ibss_uvb_L10') %>% mutate(method = 'ibss_uvb_L10')
```

```{r}
library(tidyr)

fits3 <- fits2$fit
names(fits3) <- fits2$fit_method

# bayes
logisticsusie:::cs_tbl2(fits3$bayes_ser$alpha) %>%
  unnest_longer(c(cs, prob))

logisticsusie:::cs_tbl2(fits3$uvb_ser$alpha) %>%
  unnest_longer(c(cs, prob))

logisticsusie:::cs_tbl2(fits3$vb_ser$alpha) %>%
  unnest_longer(c(cs, prob))

logisticsusie:::cs_tbl2(fits3$glm_ser$alpha) %>%
  unnest_longer(c(cs, prob))
```


Fairly good agreement between the UVB-SER PIPs and the quadrature PIPs.
```{r}
uvb_pip <- fits2 %>% filter(fit_method == 'uvb_ser') %>% pull(fit) %>% {.[[1]]$alpha}
quad_pip <- fits2 %>% filter(fit_method == 'quad_ser') %>% pull(fit) %>% {.[[1]]$alpha}

plot(log(uvb_pip), log(quad_pip)); abline(0, 1, col='red')
```

If we look at the log BFs for each variable we see that VB BFs are smaller-- also there is a "fork".
The variational BF is a better approximation for some features than others.
In earlier simulations (link?) it appeared the univariate VB bound was comparably tight across a wide range of plausible settings of the regression coefficients.
I think this picture warrants exploring how tight the bound is as we also alter the variance or sparsity of the features $X$.

```{r}
uvb_lbf <- fits2 %>% filter(fit_method == 'uvb_ser') %>% pull(fit) %>% {.[[1]]$lbf}
quad_lbf <- fits2 %>% filter(fit_method == 'quad_ser') %>% pull(fit) %>% {.[[1]]$lbf}

plot(uvb_lbf, quad_lbf); abline(0, 1, col='red')
```


### SuSiEs

Even when filtering to a smaller set of gene sets, all approximations for logistic SuSiE stuggle:

```{r}
# SuSiE
print_cs_tbl('linear_susie_L10')
print_cs_tbl('binsusie_L10')
print_cs_tbl('binsusie2_L10')
print_cs_tbl('ibss2m_L10')
print_cs_tbl('ibss_uvb_L10')
```
