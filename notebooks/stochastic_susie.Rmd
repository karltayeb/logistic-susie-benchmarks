---
title: "Stochastic SuSiE for general likelihoods"
author: "Karl Tayeb"
date: "2022-02-07"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
---

```{r}
library(tictoc)
```

## Why?

We'd like a generic way to perform inference for sparse regression with a SuSiE, *for an arbitrary likelihood*. In particular, we'd like to make it possible for a user to run a valid SuSiE regression provided an implementation of a univariate regression (satisfying some small number of requirements).

Here's the idea: it is always easy to perform inference in single effect regression (SER), because it amounts to (1) performing $p$ univariate Bayesian regressions, and (2) averaging over these $p$ models. Even for non-conjugate likelihoods, we can always perform 1d integration. In contrast, the sum of single effects regression (SuSiE) is challenging to perform inference in. The variational approximation for SuSiE $q(b, \gamma) = \prod_l q(b_l, \gamma_l)$ is sufficient to dramatically simplify inference in the linear case. However, for general likelihoods we are not so lucky, and the evaluation of the lower bound involves explicity summing over $L^p$ configurations of the selected variables.

Here we propose a stochastic optimization of the SuSiE ELBO, which let's us improve the ELBO by fitting a sequence of SERs. This let's us keep the basic "unit" of computation evaluating a univariate logistic regression, but extends SuSiE to arbitrary likelihoods. Our requirement will be that we can fit a fixed-form variational approximation to the univariate logistic regression, where $q(b)$ is constrained to be normal.


## Stochastic optimization

The SuSiE ELBO:

$$
\mathcal L(q) = \mathbb E_q\left[\log p (y | X, b, \gamma) \right] - KL\left[q(b, \gamma) || p(b, \gamma) \right]
$$

For the family that factorizes as $q(b, \gamma) = \prod_l q(b_l, \gamma_l)$, written as a function of $q_l= q(b_l, \gamma_l)$

$$
\mathcal L(q_l) = \mathbb E_{q_{-l}}\left[\mathbb E_{q_{l}}\left[
\log p (y | X, b_l, \gamma_l, b_{-l}, \gamma_{-l}) \right]\right]
- KL\left[q(b_l, \gamma_l) || p(b_l, \gamma_l)
\right] + C
$$
Where $C$ is a constant that absorbs terms not dependent on $q_l$. and $b_{-l}$,  $\gamma_{-l}$ denote the $L-1$ other SER parameters.

We can take a Monte-Carlo estimate of the outer expectation by sampling from $q_{-l}$. Let $b^{(m)}_{-l}$ and $\gamma^{(m)}_{-l}$ denote samples from $q_{-l}$, we can get an unbiased estimate of the ELBO by

$$
\hat{\mathcal L}(q_l) = \frac{1}{M} \sum \mathbb E_{q_{l}}\left[
\log p (y | X, b_l, \gamma_l, b_{-l}^{(m)}, \gamma_{-l}^{(m)}) 
- KL\left[q(b_l, \gamma_l) || p(b_l, \gamma_l)
\right]\right] + C \approx \mathcal L(q_l)
$$


Notice the term in the sum is the ELBO for a SER where $b_{-l}$ and $\gamma_{-l}$ are known. If we fit a fixed-form variational approximation for this SER $q^{(m)}(b_l, \gamma_l)$, where $q^{(m)}(b_l | \gamma_l)$ is constrained to be Gaussian (or more generally, $q^{(m)}(b | \gamma)$ and $p(b)$ are the same exponential family), then (I claim) we can get a Monte-carlo estimate of the best fixed-form approximation for $q(b_l, \gamma_l)$ by averaging the natural parameters of $q^{(m)}$.

This is true for the linear model (where the likelihood is conditionally conjugate, and the best unconstrained approximation for $q(b | \gamma)$ is Gaussian). It may take a little extra work to justify that it's the right thing to do for other likelihoods.


## Code

### Linear SER

```{r linear_ser}
linear_ser <- function(X, y, o, residual_variance=var(y), prior_variance=1, estimate_intercept=T, estimate_residual_variance=F,...){
  # form the residual, center
  if(is.list(o)) {
    # if o = list(mu, var), just take o$mu for offset
    o <- o$mu 
  }
  r <- y - o
  r <- r - mean(r)
  
  # do linear regression, assumes columns of X are mean 0
  yTx <- (r %*% X)[1,]
  xTx <- purrr::map_dbl(1:ncol(X), ~sum(X[, .x]^2))
  betahat = yTx / xTx
  
  # form final posterior
  shat2 <- residual_variance / xTx
  
  lbf <- 0.5 * log(shat2 / (prior_variance + shat2)) + (0.5 * betahat^2/shat2 * prior_variance/(prior_variance + shat2))
  alpha <- exp(lbf - matrixStats::logSumExp(lbf))
  post_var <- 1 / (1/ prior_variance + 1/shat2)
  post_mean <- post_var/shat2 * betahat
  
  res <- list(
    alpha = alpha,
    mu = post_mean,
    var = post_var,
    lbf = lbf,
    lbf_model = matrixStats::logSumExp(lbf) - log(length(lbf)),
    prior_variance=prior_variance
  )
  return(res)
}
```

### Stochastic optimization for SER/SuSiE

```{r natural_parameter_functions}
#' convert mean/variance to natural parameters
normal_mean2natural <- function(mu, var){
  eta1 <- mu/var
  eta2 <- -0.5 / var
  return(list(eta1=eta1, eta2=eta2))
}

#' convert natural parameters to mean/variance 
normal_natural2mean <- function(eta1, eta2){
  if(missing(eta2)) {
    eta2 <- eta1[2]
    eta1 <- eta1[1]
  }
  mu <- -0.5 * eta1 / eta2
  var <- mu / eta1
  return(list(mu=mu, var=var))
}

normal_cumulant <- function(mu, var){
  eta <- normal_mean2natural(mu, var)
  a <- -0.25 * eta$eta1^2/eta$eta2 - 0.5 * log(-2 * eta$eta2)
  return(a)
}

#' convert categorical probabilities to natural parameters
categorical_mean2natural <- function(pi){
  K <- length(pi)
  logpi <- log(pi)
  logpi[is.infinite(logpi)] <- -1e10
  return(head(logpi - logpi[K], K-1))
}

#' convert categorical natural parameters to probabilities
categorical_natural2mean <- function(eta){
  K <- length(eta) + 1
  eta <- c(eta, 0)
  pi <- exp(eta - matrixStats::logSumExp(eta))
  return(pi)
}


categorical_cumulant <- function(eta){
  return(matrixStats::logSumExp(c(eta, 0)))
}

#' convert mean parameters to natural paramers
#' @param q: a list with `mu`, `var` and `alpha`
#' @returns a list with `eta1`, `eta2`, and `gamma`
#'  which are the natural parameters for q(b | gamma) and q(gamma) respectively
ser_mean2natural <- function(q){
  qb <- do.call(rbind, purrr::map2(q$mu, q$var, ~unlist(normal_mean2natural(.x, .y))))
  eta_gamma <- categorical_mean2natural(q$alpha)
  q <- list(eta1 = qb[,1], eta2 = qb[,2], gamma = eta_gamma)
  return(q)
}

#' convert natural parameters to mean parameters for SER
#' @param q a list with `eta1`, `eta2`, and `gamma`
#' @returns a list with `mu`, `var` and `alpha`
ser_natural2mean <- function(qnat){
  qb <- do.call(rbind, purrr::map2(qnat$eta1, qnat$eta2, ~unlist(normal_natural2mean(.x, .y))))
  q <- list(mu = qb[,1], var = qb[,2])
  q$alpha <- categorical_natural2mean(qnat$gamma)
  return(q)
}

#' Update q SER
#' 
#' Average natural parameters of two SERs
#' Each q is an SER posterior, a list with `mu`, `var`, and `lambda`
#' @param q1 an SER posterior
#' @param q2 an SER posterior
#' @param lambda in [0, 1], how much to weight q1
update_q_ser <- function(q1nat, q2nat, lambda){
  qnat <- list(
    eta1 = lambda * q1nat$eta1 + (1 - lambda) * q2nat$eta1,
    eta2 = lambda * q1nat$eta2 + (1 - lambda) * q2nat$eta2,
    gamma = lambda * q1nat$gamma + (1 - lambda) * q2nat$gamma
  )
  return(qnat)
}
```


```{r sampling_functions}
#' Sample gamma from the SuSiE posterior
susie_sample_gamma <- function(alpha){
  L <- nrow(alpha)
  p <- ncol(alpha)
  
  gamma <- vector('numeric', L)
  for(l in 1:L){
    gamma[l] <- sample(p, 1, prob=alpha[l,])
  }
  return(gamma)
}

#' Sample b | gamma from the SuSiE posterior
susie_sample_b <- function(gamma, mu, var){
  L <- length(gamma)
  b <- vector('numeric', L)
  for(l in 1:L){
    b[l] <- rnorm(1, mean=mu[l, gamma[l]], sd=sqrt(var[l, gamma[l]]))
  }
  return(b)
}

#' Sample offset
#' 
#' Computes a Monte-Carlo sample of X E[b]
#' where `b` is sampled from `fit`
#' @param fit a susie fit (minimally, a list with `alpha`, `mu`, `var`)
#' @param X an n x p matrix
#' @l a single effect to exclude/zero-out (defalt 0, don't remove any single effect)
#' @sample_b whether to sample `b_l` if false
#' @return an offset, which is a list containing `mu` and `var`.
#'  If `sample_b=T` `mu` is the sample predictions, and `var` is a vector of 0s
#'  Otherwise `mu` is the E_q[Xb | gamma] and `var` is Var_q[Xb | gamma]
sample_offset <- function(fit, X, l=0, sample_b=T){
  gamma <- with(fit, susie_sample_gamma(alpha))
  L <- length(gamma)
  if(sample_b){
    b <- with(fit, susie_sample_b(gamma, mu, var))
    b[l] <- 0  # 0 out the effect (note if l=0 this does nothing)
    o <- list(mu = (X[, gamma] %*% b)[, 1], var = rep(0, nrow(X)))
  } else {
    mu <- purrr::map_dbl(1:L, ~fit$mu[.x, gamma[.x]])
    var <- purrr::map_dbl(1:L, ~fit$var[.x, gamma[.x]])
    mu[l] <- 0  # 0 out effect l
    var[l] <- 0
    
    mu <- (X[, gamma] %*% mu)[, 1]
    mu2 <- (X[, gamma]^2 %*% var)[,1] + mu^2
    o <- list(mu = mu, mu2=mu2)
  }
  return(o)
}

#' Initialize q SER
#' 
#' A simple initialization for the SER posterior, initialize to prior
#' @param p number of features
#' @param prior_variance prior variance of the effect
initialize_q_ser <- function(p, prior_variance=1){
  post <- list(
    mu = rep(0, p),
    var = rep(prior_variance, p),
    alpha = rep(1/p, p)
  )
  return(post)
}
```

```{r kl_functions}
kl_ser <- function(q1, q2, compute_kl_gamma = T, compute_kl_b = T){
  kl <- ifelse(
    compute_kl_gamma,
    logisticsusie:::categorical_kl(q1$alpha, q2$alpha),
    0
  )
  kl <- ifelse(
    compute_kl_b,
    kl + sum(q1$alpha * logisticsusie:::normal_kl(q1$mu, q1$var, q2$mu, q2$var)),
    kl
  )
  return(kl)
}

kl_ser_sym <- function(q1, q2, ...){
  kl_ser(q1, q2, ...) + kl_ser(q2, q1, ...)
}
```


```{r stochastic_ser_function}
#' take an average of the natural parameters
average_q <- function(qs, history=T){
  post_history <- list()
  M <- length(qs)
  
  # record natural parameter
  post <- ser_mean2natural(qs[[1]])
  post_history[[1]] <- qs[[1]]
  for (m in 2:M){
    # combine natural parameters in on-line average
    qnat <- ser_mean2natural(qs[[m]])
    post <- update_q_ser(post, qnat, 1 - 1/m)
    
    if(history){
      post_history[[m]] <- ser_natural2mean(post)
    }
  }
  
  if(history){
    return(post_history)
  } else{
    # report mean parameterization
    return(ser_natural2mean(post))
  }
}

#' Take a montecarlo sample of q_{-l} and fit an SER
mc_sample_ser <- function(X, y, fit, l, ser_fun, sample_b=T){
  o <- sample_offset(fit, X, l=l, sample_b = sample_b)
  post <- ser_fun(X, y, o)
  return(post)
}

#' Samples from fit, and fits an SER with the offset
#' then combines the results by averaging natural parameters
stochastic_ser <- function(X, y, fit, l, M=100, q_init=NULL, history=F, ser_fun=NULL, sample_b=T){
  # 0: setup
  p <- ncol(fit$alpha)
  if(is.null(ser_fun)){
    ser_fun = linear_ser    # if no ser specified, default to linear
  }
  
  # 1. fit M SERs (could parallelize, or save memory by doing sequentially)
  post <- list()
  for (i in 1:M){
    post[[i]] <- with(data, mc_sample_ser(X, y, fit, l, ser_fun, sample_b = sample_b))
  }

  # 1. combine by averaging natural parameters
  q_bar <- average_q(post, history=T)
  return(q_bar)
}

```


```{r fisher_weighting_function}
# these are supposed to use fisher information to weight/combine SERs

normal_moments <- function(mu, var, k){
  if(k == 1){
    return(mu)
  } else if (k == 2){
    return(var + mu^2)
  }
  
  return (mu * normal_moments(mu, var, k-1) + 
            (k-1) * var * normal_moments(mu, var, k-2))
}

normal_compute_I <- function(var){
  c(1/var, 1/(2 * var^2))
}

categorical_compute_I <- function(pi){
  pi <- head(pi, length(pi) - 1)
  I <- diag(pi) - outer(pi, pi)
  return(I)
}

update_qgamma <- function(I, Ieta, pi, compute_eta=F, diag=F){
  pi_trunc <- pmin(pmax(pi, 0.001), 0.999)
  pi_trunc <- pi_trunc / sum(pi_trunc)
  pi <- pi_trunc
  
  if(diag){
    newI = head(pi * (1-pi), length(pi-1))
    neweta <- categorical_mean2natural(pi)
    newIeta <- newI * neweta
  } else{
    newI <- categorical_compute_I(pi)
    neweta <- categorical_mean2natural(pi)
    newIeta <- (newI %*% neweta)[, 1]
  }
  
  I <- I + newI
  Ieta <- Ieta + newIeta
  
  if(compute_eta){
    if(diag){
      eta <- Ieta / eta
    } else{
      eta <- solve(I, Ieta)
    }
  } else{
    eta <- NULL
  }
  return(list(I = I, Ieta = Ieta, eta=eta))
}

average_q2 <- function(qs, history=T, diag=F){
  post_history <- list()
  M <- length(qs)
  
  post <- qs[[1]]
  
  if(diag){
    pi <- pmin(pmax(post$alpha, 0.01), 0.99)
    I <- head(post$alpha * (1 - pi), length(pi) - 1)
    eta <- categorical_mean2natural(pi)
    Ieta <- I * eta
  } else{
    I <- categorical_compute_I(post$alpha)
    eta <- categorical_mean2natural(post$alpha)
    Ieta <- (I %*% eta)[,1]
  }
  gamma_post <- list(I = I, Ieta = Ieta, eta = eta)
  
  post_history[[1]] <- post
  for (m in 2:M){
    post <- update_q_ser(post, qs[[m]], 1 - 1/m)
    gamma_post <- update_qgamma(gamma_post$I, gamma_post$Ieta, qs[[m]]$alpha, compute_eta=history)
    if(history){
      post$alpha <- categorical_natural2mean(gamma_post$eta)
      post_history[[m]] <- post
    }
  }
  
  if(history){
    return(post_history)
  } else{
    return(post)
  }
}
```


Use stochastic SER to make stochastic SuSiE

```{r stochastic_susie_functions}
update_fit <- function(fit, ser, l){
  fit$alpha[l,] <- ser$alpha
  fit$mu[l,] <- ser$mu
  fit$var[l,] <- ser$var
  return(fit)
}

stochastic_susie <- function(X, y, L, prior_variance=1., niter=100, ser_iter = 100, history=F, ser_fun=NULL, sample_b=T){
  # 0: setup
  n <- length(y)
  p <- ncol(X)
  
  # initialize
  fit <- list(
    alpha = matrix(rep(1/p, p*L), nrow = L),
    mu = matrix(rep(0, p*L), nrow = L),
    var = matrix(rep(prior_variance, p*L), nrow = L)
  )
  post_history <- list()
  
  # initialize_sers
  sers <- purrr::map(1:L, ~initialize_q_ser(p))
  
  for (iter in 1: niter){
    for (l in 1:L){
      # TODO: set q_init = sers[[l]]?
      # right now it doesnt make much difference, only weighed as 1 sample
      ser_l <- stochastic_ser(X, y, fit, l, M=ser_iter, ser_fun=ser_fun, sample_b = sample_b)
      sers[[l]] <- ser_l
      fit <- update_fit(fit, ser_l, l)
    }
    if(history){
      post_history[[iter]] <- fit
    }
  }
  if(history){
    return(post_history)
  } else{
    return(fit)
  }
}

susie_plot2 <- function(fit, X, y = "PIP", ...) {
  fit$pip <- logisticsusie:::compute_pip(fit$alpha)
  fit$sets <- susieR::susie_get_cs(fit, X = X)
  class(fit) <- "susie"
  susieR::susie_plot(fit, y, ...)
}
```


## SER Examples

### Helpful functions 

```{r stochast_ser_trace}
stochastic_ser_trace <- function(data, susie_fit, which_l, M=5000, seed=NULL, ser_fun=linear_ser, sample_b=T){
  if(!is.null(seed)){
      set.seed(seed)
  }
  
  # 0. extract exact SER posterior
  susie_l <- list(
    alpha=susie_fit$alpha[which_l,],
    mu=susie_fit$mu[which_l,],
    var=susie_fit$var[which_l,]
  )
  
  # 1. Fit M MC SERs
  post <- list()
  for (i in 1:M){
    post[[i]] <- with(data, mc_sample_ser(X, y, susie_fit, which_l, ser_fun, sample_b = sample_b))
  }
  
  # 2. combine by averaging 
  q_bar <- average_q(post, history=T)
  
  # 3. compute KL to exact posterior
  
  kl <- purrr::map_dbl(1:length(q_bar), ~ kl_ser_sym(susie_l, q_bar[[.x]]))
  kl_gamma <- purrr::map_dbl(1:length(q_bar), ~ kl_ser_sym(susie_l, q_bar[[.x]], compute_kl_b=F))
  
  return(list(mc_ser = post, q_bar = q_bar, kl_gamma = kl_gamma, kl = kl, q_ser = susie_l))
}
```

```{r kl_plot}
plot_kl <- function(kls, colors, labels, xp=0.7, yp=0.1, ...){
  yrange <- range(log10(unlist(kls) + 1e-20))

  plot(log10(kls[[1]]),
       col = colors[1],
       xlab='iter',
       ylab = 'log10(KL)',
       type = 'line',
       ylim = yrange,
       ...
  )
  
  if(length(kls) > 1){
    for (i in 2:length(kls)){
      lines(log10(kls[[i]]), col=colors[i])
    }
  }

  
  # Add a legend
  legend(
    x = length(kls[[1]]) * xp,
    y = yrange[2] - abs(diff(yrange)) * yp,
    legend = labels,
    lty = rep('solid', length(colors)),
    col = colors,
    cex=0.8
  )
}
```

### Recovering a single SuSiE component: stochastic approdimation for Linear SER

We use a toy simulation with 3 effects, and fit linear SuSiE as a baseline. Our goal here is to estimate $q(b_3, \gamma_3)$ using our stochastic approach. We fix the residual variance $\sigma^2 = Var(y)$ where $Var(y)$ is the sample variance, and the prior variance $\sigma^2_0 = 1$. We see that as we average the natural parameters across many draws of $\gamma_{-l}$, or $\{\gamma_{-l}, b_{-l}\}$ we approach the exact SER solution. Unsurprisingly, when we only sample over configurations, the convergence is much faster.

```{r sim1}
# simulate data
set.seed(5)
sim <- logisticsusie::sim_susie(length_scale = 10)

# standardize data
data <- with(sim, {
  y <- y - mean(y)
  X <- scale(X)
  list(y=y, X=X)
})

# fit SuSiE
susie_fit <- with(data, susieR::susie(
  X, y, L = 5, scaled_prior_variance = 1, estimate_prior_variance = F, estimate_residual_variance = F))
susieR::susie_plot(susie_fit, 'PIP')
susie_fit$var <- susie_fit$mu2 - susie_fit$mu^2
```

#### Recover each SER

```{r sim1_stochastic_ser_kl_gamma}
ser1 <- stochastic_ser_trace(data, susie_fit, 1)
ser2 <- stochastic_ser_trace(data, susie_fit, 2)
ser3 <- stochastic_ser_trace(data, susie_fit, 3)

plot_kl(
  list(ser1$kl_gamma, ser2$kl_gamma, ser3$kl_gamma),
  c('purple', 'green', 'blue'),
  c('q(gamma_1)', 'q(gamma_2)', 'q(gamma_3)'),
  main = 'Symmetrized KL to exact SER posterior'
)
```

#### Look at full KL

There is a minor discrepency between the way we compute the posterior variance
and it's implimentation in SuSiE that keeps the `q(b | gamma)` component away from 0

```{r sim1_stochastic_ser}
set.seed(10)
M <- 5000
which_l <- 3

# 0. extract exact SER posterior
susie_l <- list(
  alpha=susie_fit$alpha[which_l,],
  mu=susie_fit$mu[which_l,],
  var=susie_fit$var[which_l,]
)

# 1. Fit M MC SERs
post <- list()
for (i in 1:M){
  post[[i]] <- with(data, mc_sample_ser(X, y, susie_fit, which_l, linear_ser, sample_b = F))
}

# 2. combine by averaging 
q_bar <- average_q(post, history=T)

# 3. compute KL to exact posterior
kl_bar <- purrr::map_dbl(1:length(q_bar), ~ kl_ser_sym(susie_l, q_bar[[.x]]))
kl_bar_b <- purrr::map_dbl(1:length(q_bar), ~ kl_ser_sym(susie_l, q_bar[[.x]], compute_kl_gamma=F))
kl_bar_gamma <- purrr::map_dbl(1:length(q_bar), ~ kl_ser_sym(susie_l, q_bar[[.x]], compute_kl_b=F))

plot_kl(
  list(kl_bar, kl_bar - kl_bar_gamma, kl_bar_gamma),
  c('red', 'blue', 'purple'),
  c('stochastic SER', 'E[q(b | gamma)]', 'q(gamma)'),
  main = 'Symmetrized KL to exact SER posterior'
)
```



### A harder example

```{r ser_sim2}
# simulate data
set.seed(110)
sim <- logisticsusie::sim_susie(p=100, length_scale = 100, idx = c(20, 40, 60))

# standardize data
data <- with(sim, {
  y <- y - mean(y)
  X <- scale(X)
  list(y=y, X=X)
})

# fit SuSiE
susie_fit <- with(data, susieR::susie(
  X, y, L = 5, scaled_prior_variance = 1, estimate_prior_variance = F, estimate_residual_variance = F))
susieR::susie_plot(susie_fit, 'PIP')
susie_fit$var <- susie_fit$mu2 - susie_fit$mu^2
```

```{r}
ser1 <- stochastic_ser_trace(data, susie_fit, 1)
ser2 <- stochastic_ser_trace(data, susie_fit, 2)
ser3 <- stochastic_ser_trace(data, susie_fit, 3)

plot_kl(
  list(ser1$kl_gamma, ser2$kl_gamma, ser3$kl_gamma),
  c('purple', 'green', 'blue'),
  c('q(gamma_1)', 'q(gamma_2)', 'q(gamma_3)'),
  main = 'Symmetrized KL to exact SER posterior'
)
```

#### Integrate over effect size

Integrating over effects seems to have less of an impact in this harder example?

```{r}
ser1 <- stochastic_ser_trace(data, susie_fit, 1, sample_b=F, M=2000)
ser2 <- stochastic_ser_trace(data, susie_fit, 1, sample_b=T, M=2000)

plot_kl(
  list(ser1$kl_gamma, ser2$kl_gamma),
  c('blue', 'red'),
  c('E[b]', 'sample b'),
  main = 'Symmetrized KL to exact SER posterior'
)
```

```{r ser_sim2_mc_parameters}
q_sample_b <- ser1$mc_ser[[1]]
q_Eb <- ser2$mc_ser[[1]]
q_ser <- ser1$q_ser

par(mfrow = c(1, 2))
plot(log(q_sample_b$alpha),
     log(q_ser$alpha),
     xlab = 'Stochastic Approximation',
     ylab = 'SER',
     main = 'sample b - log(alpha)')
abline(0, 1, col='red')

plot(log(q_Eb$alpha),
     log(q_ser$alpha),
     xlab = 'Stochastic Approximation',
     ylab = 'SER',
     main = ' E[b] - log(alpha)')
abline(0, 1, col='red')

par(mfrow = c(1, 2))
plot(q_sample_b$mu,
     q_ser$mu,
     xlab = 'Stochastic Approximation',
     ylab = 'SER',
     main = 'sample b - mu')
abline(0, 1, col='red')

plot(q_Eb$mu,
     q_ser$mu,
     xlab = 'Stochastic Approximation',
     ylab = 'SER',
     main = 'E[b] - mu')
abline(0, 1, col='red')
```


```{r ser_sim2_posterior_parameters}
m <- 1000
q_sample_b <- ser1$q_bar[[m]]
q_Eb <- ser2$q_bar[[m]]
q_ser <- ser1$q_ser

par(mfrow = c(1, 2))
plot(log(q_sample_b$alpha),
     log(q_ser$alpha),
     xlab = 'Stochastic Approximation',
     ylab = 'SER',
     main = 'sample b - log(alpha)')
abline(0, 1, col='red')

plot(log(q_Eb$alpha),
     log(q_ser$alpha),
     xlab = 'Stochastic Approximation',
     ylab = 'SER',
     main = ' E[b] - log(alpha)')
abline(0, 1, col='red')

par(mfrow = c(1, 2))
plot(q_sample_b$mu,
     q_ser$mu,
     xlab = 'Stochastic Approximation',
     ylab = 'SER',
     main = 'sample b - mu')
abline(0, 1, col='red')

plot(q_Eb$mu,
     q_ser$mu,
     xlab = 'Stochastic Approximation',
     ylab = 'SER',
     main = 'E[b] - mu')
abline(0, 1, col='red')
```


### Logistic SER

```{r logistic_susie_ser}
# simulate data
set.seed(13)
sim <- logisticsusie::sim_susie(length_scale = 20)

# standardize data
data <- with(sim, {
  y <- y
  X <- scale(X)
  list(y=y, X=X)
})

tic('ibss')
ibss_fit <- with(data, logisticsusie::ibss_from_ser2(
  X, y, L=5, prior_variance = 1, ser_function = logisticsusie::fit_uvb_ser))
toc()

susie_plot2(ibss_fit, data$X)
```

```{r}
fit_uvb_ser2 <- function(X, y, o){logisticsusie::fit_uvb_ser(X, y, o$mu)}

fit_uvb_ser_re <- logisticsusie::fit_uvb_ser_re # needs `sample_b = F`


# with(data, mc_sample_ser(X, y, ibss_fit, 3, fit_uvb_ser_re, sample_b = F))
# with(data, mc_sample_ser(X, y, ibss_fit, 3, fit_uvb_ser2, sample_b = T))
# with(data, mc_sample_ser(X, y, ibss_fit, 3, fit_uvb_ser2, sample_b = F))

# uvb, sample b
tic()
ser1 <- stochastic_ser_trace(data, ibss_fit, 2, ser_fun=fit_uvb_ser2, M=1000, sample_b = T)
toc()

tic()
ser2 <- stochastic_ser_trace(data, ibss_fit, 2, ser_fun=fit_uvb_ser2, M=1000, sample_b = T)
toc()

tic()
ser3 <- stochastic_ser_trace(data, ibss_fit, 3, ser_fun=fit_uvb_ser2, M=1000, sample_b = T)
toc()

plot_kl(
  list(ser1$kl_gamma, ser2$kl_gamma, ser3$kl_gamma),
  c('purple', 'green', 'blue'),
  c('q(gamma_1)', 'q(gamma_2)', 'q(gamma_3)'),
  main = 'Symmetrized KL to exact SER posterior'
)
```

```{r logistic_ser_compar_posterior}
m <- 1000
q_sample_b <- ser1$q_bar[[m]]
q_ser <- ser1$q_ser

par(mfrow = c(1, 2))
plot(log(q_sample_b$alpha),
     log(q_ser$alpha),
     xlab = 'Stochastic Approximation',
     ylab = 'SER',
     main = 'log(alpha)')
abline(0, 1, col='red')

plot(q_sample_b$mu,
     q_ser$mu,
     xlab = 'Stochastic Approximation',
     ylab = 'SER',
     main = 'mu')
abline(0, 1, col='red')
```




## SuSiE Examples

### Linear SuSiE Test

```{r sim2_susie}
library(tictoc)

# simulate data
set.seed(10)
sim <- logisticsusie::sim_susie(length_scale = 20)

# standardize data
data <- with(sim, {
  y <- y - mean(y)
  X <- scale(X)
  list(y=y, X=X)
})

tic('ibss')
ibss_fit <- with(data, logisticsusie::ibss_from_ser2(
  X, y, L=5, prior_variance = 1, ser_function = linear_ser))
toc()

tic('stochastic ibss')
stochastic_fit_b <- with(data, stochastic_susie(
  X, y, L=5, niter = 20, ser_iter = 100, history = T, sample_b = T))
toc()

tic('stochastic ibss')
stochastic_fit_Eb <- with(data, stochastic_susie(
  X, y, L=5, niter = 20, ser_iter = 100, history = T, sample_b = F))
toc()

tic('susie')
susie_fit <- with(data, susieR::susie(
  X, y, L = 5, scaled_prior_variance = 1, estimate_prior_variance = F))
toc()
```


```{r}
par(mfrow = c(1, 3))
susie_plot2(stochastic_fit_b[[20]], data$X, main = 'Stochastic - sample b')
susie_plot2(stochastic_fit_Eb[[20]], data$X, main = 'Stochastic - E[b]')
susie_plot2(ibss_fit, data$X, main =' SuSiE')
```

### Logistic SuSiE Test

On a small example, we see comparable performance between our stochastic approximation moment-mathcing

```{r sim3_logistic_susie}
library(tictoc)

# simulate data
set.seed(12)
sim <- logisticsusie::sim_susie(length_scale = 20)

# standardize X
data <- with(sim, {
  y <- y
  X <- scale(X)
  list(y=y, X=X)
})

tic('ibss')
ibss_uvb_fit <- with(data, logisticsusie::ibss_from_ser2(
  X, y, L=5, prior_variance = 1, ser_function = logisticsusie::fit_uvb_ser))
toc()

tic('stochastic uvb')
stochastic_uvb_re_fit <- with(data, stochastic_susie(
  X, y, L=5, niter = 10, ser_iter = 10, history = T, ser_fun = logisticsusie::fit_uvb_ser_re, sample_b = F))
toc()
```


```{r}
par(mfrow = c(1, 2))
susie_plot2(stochastic_uvb_re_fit[[10]], data$X, main = 'Stochastic - UVB-RE')
susie_plot2(ibss_uvb_fit, data$X, main ='IBSS-UVB')
```



### Poisson regression test

```{r}

```




