---
title: "Constant Simulation Results"
author: "Karl Tayeb"
date: "2022-12-01"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
    toc_collapsed: true
    toc_depth: 3
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(error = TRUE)
```

```{r imports}
library(dplyr)
library(tidyr)
library(targets)
library(ggplot2)
```

```{r load_data}
# get pipsi
pips <- tar_read(constant_pips)
X_order <- unique(pips$X_name)[c(3,1,2)]
pips$X_name <- factor(pips$X_name, levels = X_order)

cs <- tar_read(constant_cs)
cs$X_name <- factor(cs$X_name, levels = X_order) 
```

## Overview

### Simulations

We assess the power and calibration of PIPs, as well as coverage of 95% credible sets, for various SERs and SuSiEs.

We simulate `n=500` samples and `p=100` features. We simulate a binary design matrix `X`, to simulate the gene set enrichment scenario.
Each column of `X` has 20% of it's entries set to 1. We control the correlation between adjacent columns of `X` by  copying the previous column
and randomly flipping a small proportion of 1's to 0's (`p_flip`), and a corresponding number of 0's to 1's. By setting `p_flip` to a small value,
we can generate highly correlated `X`, conversely setting it to large value quickly decouples the features of `X`. We generate `X` with `p_flip = 0.02, 0.1, 0.5`
which we call `X_bin_strong`, `X_bin_medium` and `X_bin_weak` respectively, to explore the performance of our variable selection methods across a range of correlation strenghts.


We generate binary observatons `y` under a simple model. We select $L$ columns of `X` to be enriched.
If $x_{ij} = 1$ for at least 1 of the enriched columns, $y_i = 1$ with probability $p_{active}$,
otherwise $y_i =1$ with probability $p_{background}$. 
When $L=1$ this corresponds with an additive model on the log-odds scale. 

We simulate under `background_logit = -10, -4, -2`. and `active_logit = -1, -0.5, 0`,
selecting `L=1, 2, 3` enriched features. We repeat each simulation setting for 20 iterations.


### Methods Assessed

We assess the performance of the linear SER and several approximations to the logistic SER.
We also assess the performance of several SuSiE variants. As a baseline we run linear SuSiE.
We fit logistic SuSiE with the global variational approximation, IBSS, and IBSS2M.

## SERs
### PIP Calibration
We report the calibration of posterior inclusion probabilities (PIPs). The PIP summarizes the marginaly probability that each feature is included in the model.
We bin features across all simulations by there PIPs, and assess the frequency that features in each PIP bin are actually enriched.
If thre observed frequency closely matches the average value of PIPs in each bin, we say that the PIPs are well calibrated.

Overall the Univariate VB SER shows the best calibration, even being slightly conservative at times.
The VB SER may have slightly anti-conservative PIPs, which is consistent with our expectation that
the variational approximation will tend to favor whichever feature has the strongest evidence of association.

#### SERs, $L=1$
```{r}
pips %>%
  filter_ser() %>%
  filter(L==1) %>%
  make_pip_calibration_plot() +
  facet_wrap(vars(fit_method))
```

#### SERs, $L > 1$
```{r}
pips %>%
  filter_ser() %>%
  filter(L>1) %>%
  make_pip_calibration_plot() +
  facet_wrap(vars(fit_method))
```

### FDP vs Power

In the single effect simulations we see that the Univariate VB SER has greater power than the VB SER, and comparable performance to the linear SER. 
Since among equally powered approaches we prefer the better calibrated one, the UVB SER shows the best performance here.

When there are multiple effects we see that the performance of the linear model takes a hit.
The VB SER does a slightly better job ordering PIPs close to 1, but overall performance is comparable.

#### $L=1$
```{r}
pips %>%
  filter_ser() %>%
  filter(L==1) %>%
  make_fdp_plot()
```

#### $L>1$

```{r}
pips %>%
  filter_ser() %>%
  filter(L>1) %>%
  make_fdp_plot()
```

### 95% credible set coverage 

As the correlation structure in the design matrix increases, and as the number of enriched features increases we see a loss of coverage in all models.
The credible sets from the UVB SER tend to have better coverage than the other methods.
We note that the loss of coverage in the VB model as correlation structure increases is expected.
Loss of coverage when $L>1$, and in the linear model, may be attributable to model mis-specification.

#### All credible sets 
```{r}
cs %>%
  filter_ser() %>%
  make_cs_coverage_plot() +
  facet_wrap(vars(X_name))
```

#### Credible sets with $\leq 25$ feeatures
```{r}
cs %>%
  filter_ser() %>%
  make_cs_coverage_plot(max_cs_size = 25) +
  facet_wrap(vars(X_name))
```


## SuSiE
### PIP Calibration
Calibration of PIPs in the single-effect simulations

#### $L=1$
```{r}
pips %>%
  filter_susie() %>%
  filter(L==1) %>%
  make_pip_calibration_plot() +
  facet_wrap(vars(fit_method))
```

#### $L > 1$
```{r}
pips %>%
  filter_susie() %>%
  filter(L>1) %>%
  make_pip_calibration_plot() +
  facet_wrap(vars(fit_method))
```

### FDP vs Power
```{r}
pips %>%
  filter_susie() %>%
  filter(L==1) %>%
  make_fdp_plot()
```

```{r}
pips %>%
  filter_susie() %>%
  filter(L>1) %>%
  make_fdp_plot()
```

### 95% credible set coverage 

IBSS2M is the clear winner here. We have good coverage across all `X` and for multiple enriched features.
We note that the suprisingly low coverage of the other methods when assessing all credible sets is a bit misleading--
in practice we fit SuSiE with more components that needed, and this leads to a fit model with uninformative components. What we are seeing here is that when SuSiE correctly identifies a feature in one component,
the uninformative components are less likely to include that enriched feature.
Even crude filtering of the credible sets remedies this. For example for `X_bin_weak` we see that all "small" credible sets (ie probably not a null component) have good empirical coverage.

#### All credible sets
```{r}
cs %>%
  filter_susie() %>%
  make_cs_coverage_plot() +
  facet_wrap(vars(X_name))
```

#### Credible sets with $\leq 25$ features
```{r}
cs %>%
  filter_susie() %>%
  make_cs_coverage_plot(max_cs_size = 25) +
  facet_wrap(vars(X_name))
```
